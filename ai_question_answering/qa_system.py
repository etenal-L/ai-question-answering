from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA

def run_qa_system(persist_directory="./chroma_db"):
    # 1ï¸âƒ£ åŠ è½½ Chroma æ•°æ®åº“
    embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-small-zh-v1.5")
    vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)

    # 2ï¸âƒ£ è®¾ç½® Retriever
    retriever = vectordb.as_retriever()

    # 3ï¸âƒ£ é…ç½® Ollama ï¼ˆDeepSeek æ¨¡å‹ï¼‰
    llm = Ollama(model="r1")  # è¿™é‡Œç”¨ deepseek-8b æ¨¡å‹ï¼ˆå¿…é¡»å·²åœ¨æœ¬åœ° Ollama å¯ç”¨ï¼‰

    # 4ï¸âƒ£ æ„å»º RetrievalQA
    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

    print("âœ… é—®ç­”ç³»ç»Ÿå·²å¯åŠ¨ã€‚è¯·è¾“å…¥é—®é¢˜ï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰ï¼š")
    while True:
        question = input("ğŸ“ ä½ çš„é—®é¢˜ï¼š")
        if question.lower() == "exit":
            break
        answer = qa_chain.run(question)
        print("ğŸ¤– AI å›ç­”ï¼š", answer)

if __name__ == "__main__":
    run_qa_system()
